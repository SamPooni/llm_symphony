<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Tokenization Deep Dive - BPE, WordPiece, SentencePiece</title>
  <script src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
  <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body { background: #000; min-height: 100vh; }
    ::-webkit-scrollbar { width: 8px; }
    ::-webkit-scrollbar-track { background: #1e293b; }
    ::-webkit-scrollbar-thumb { background: #475569; border-radius: 4px; }
  </style>
</head>
<body>
  <div id="root"></div>
  
  <script type="text/babel">
    const { useState } = React;

    function TokenizationDiagram() {
      const [tooltip, setTooltip] = useState(null);
      const [mousePos, setMousePos] = useState({ x: 0, y: 0 });

      const handleMouseMove = (e) => {
        setMousePos({ x: e.clientX, y: e.clientY });
      };

      const showTooltip = (id) => setTooltip(id);
      const hideTooltip = () => setTooltip(null);

      const tooltips = {
        // WHY TOKENIZATION
        'why-tokenize': {
          title: 'üéØ WHY TOKENIZE?',
          content: 'Neural networks need numbers, not text. Tokenization bridges the gap.',
          process: 'Text ‚Üí Tokens ‚Üí Token IDs ‚Üí Embeddings\n"Hello" ‚Üí ["Hel", "lo"] ‚Üí [1234, 5678] ‚Üí [vector, vector]',
          key: 'The vocabulary and tokenization strategy affect model quality, efficiency, and multilingual capability.'
        },
        'why-not-chars': {
          title: '‚ùå WHY NOT CHARACTERS?',
          content: 'Character-level is too fine-grained. Sequences become very long.',
          problem: '"Hello world" = 11 characters\nLong sequences = more compute\nHard to learn word meanings from chars',
          example: 'A 1000-word document:\nCharacters: ~5000 tokens\nSubwords: ~1300 tokens\n4√ó more efficient!'
        },
        'why-not-words': {
          title: '‚ùå WHY NOT WORDS?',
          content: 'Word-level can\'t handle unknown words (OOV problem).',
          problem: 'Vocabulary explodes (millions of words)\nCan\'t handle: typos, new words, compounds\n"ChatGPT" ‚Üí [UNK] if not in vocab',
          example: 'English has 170,000+ words\nPlus names, technical terms, slang...\nImpossible to cover all!'
        },
        'why-subwords': {
          title: '‚úÖ SUBWORD TOKENIZATION',
          content: 'Best of both worlds: common words stay whole, rare words split.',
          benefit: 'Fixed vocabulary (32K-256K)\nNo OOV problem (can spell anything)\nEfficient sequences\nMorphologically aware',
          example: '"unhappiness" ‚Üí ["un", "happiness"]\n"ChatGPT" ‚Üí ["Chat", "G", "PT"]'
        },

        // BPE
        'bpe-concept': {
          title: 'üî§ BYTE PAIR ENCODING (BPE)',
          content: 'Start with characters, iteratively merge most frequent pairs.',
          origin: 'Originally a compression algorithm (1994)\nAdapted for NLP by Sennrich et al. (2016)\nUsed by GPT, LLaMA, Mistral',
          key: 'Data-driven: Learns vocabulary from corpus'
        },
        'bpe-training': {
          title: 'üéì BPE TRAINING',
          content: 'Build vocabulary by repeatedly merging frequent character pairs.',
          steps: '1. Start with character vocabulary\n2. Count all adjacent pairs\n3. Merge most frequent pair\n4. Add merged token to vocab\n5. Repeat until vocab_size reached',
          example: 'Corpus: "low lower lowest"\nPairs: (l,o)=3, (o,w)=3, (w,e)=2...\nMerge "lo" ‚Üí vocab: [..., "lo"]\nNow: "lo" "w" "lo" "w" "er"...'
        },
        'bpe-merge-rules': {
          title: 'üìã MERGE RULES',
          content: 'Ordered list of merges learned during training.',
          structure: 'Rule 1: (e, s) ‚Üí "es"\nRule 2: (l, o) ‚Üí "lo"\nRule 3: (lo, w) ‚Üí "low"\n...',
          application: 'Apply rules in order during encoding\nEarlier rules = more common patterns\nOrder matters for consistency!'
        },
        'bpe-encoding': {
          title: 'üî¢ BPE ENCODING',
          content: 'Apply learned merge rules to tokenize new text.',
          process: '1. Split text into characters\n2. Apply merge rules in order\n3. Stop when no more rules apply\n4. Map tokens to IDs',
          example: '"lowest" ‚Üí [l,o,w,e,s,t]\nApply (l,o)‚Üílo: [lo,w,e,s,t]\nApply (lo,w)‚Üílow: [low,e,s,t]\nApply (e,s)‚Üíes: [low,es,t]\nFinal: ["low", "es", "t"]'
        },
        'bpe-decoding': {
          title: 'üîô BPE DECODING',
          content: 'Convert token IDs back to text. Simple concatenation.',
          process: 'IDs ‚Üí Tokens ‚Üí Concatenate\n[1234, 5678, 9012] ‚Üí ["low", "es", "t"] ‚Üí "lowest"',
          note: 'Decoding is trivial: just concat\nNo information loss (lossless)'
        },

        // BYTE-LEVEL BPE
        'bbpe-concept': {
          title: 'üî¢ BYTE-LEVEL BPE',
          content: 'Operate on bytes (0-255) instead of Unicode characters.',
          why: 'Unicode has 150,000+ characters\nBytes: Only 256 base tokens!\nCan encode ANY text (any language, emoji, binary)',
          used: 'GPT-2, GPT-3, GPT-4, LLaMA, Mistral'
        },
        'bbpe-base-vocab': {
          title: 'üìä BASE VOCABULARY',
          content: '256 byte tokens form the base vocabulary.',
          structure: 'Tokens 0-255: Individual bytes\nTokens 256+: Merged byte sequences\nTotal vocab: 32K-256K typical',
          benefit: 'Never OOV - can represent any byte sequence\nMultilingual by default'
        },
        'bbpe-utf8': {
          title: 'üåê UTF-8 ENCODING',
          content: 'Text first converted to UTF-8 bytes, then tokenized.',
          utf8: 'ASCII (a-z): 1 byte each\nLatin extended: 2 bytes\nCJK characters: 3 bytes\nEmoji: 4 bytes',
          implication: 'Non-ASCII text uses more tokens\nChinese/Japanese less efficient than English'
        },

        // WORDPIECE
        'wordpiece-concept': {
          title: 'üî§ WORDPIECE',
          content: 'Similar to BPE but uses likelihood instead of frequency.',
          origin: 'Developed by Google for BERT\nOptimizes for language model likelihood\nSlightly different merge criterion',
          marker: 'Uses ## prefix for continuation\n"playing" ‚Üí ["play", "##ing"]'
        },
        'wordpiece-training': {
          title: 'üéì WORDPIECE TRAINING',
          content: 'Merge pairs that maximize language model likelihood.',
          criterion: 'Score = freq(ab) / (freq(a) √ó freq(b))\nHigher score = more "surprising" combination\nFavors pairs that co-occur more than expected',
          difference: 'BPE: Pure frequency\nWordPiece: Normalized by individual frequencies'
        },
        'wordpiece-prefix': {
          title: '## PREFIX MARKER',
          content: 'Marks tokens that continue a word (not word-start).',
          example: '"playing" ‚Üí ["play", "##ing"]\n"unhappy" ‚Üí ["un", "##happy"]\n"ChatGPT" ‚Üí ["Chat", "##G", "##PT"]',
          benefit: 'Preserves word boundary information\nModel knows if token starts a word'
        },

        // SENTENCEPIECE
        'sp-concept': {
          title: 'üî§ SENTENCEPIECE',
          content: 'Language-agnostic tokenizer. Treats input as raw bytes/chars.',
          key: 'No pre-tokenization (no word splitting first)\nTreats space as a character (‚ñÅ)\nWorks identically for all languages',
          used: 'T5, ALBERT, mBART, LLaMA, Mistral'
        },
        'sp-unigram': {
          title: 'üìä UNIGRAM MODEL',
          content: 'SentencePiece can use Unigram LM instead of BPE.',
          approach: 'Start with LARGE vocabulary\nIteratively REMOVE tokens\nKeep tokens that minimize loss',
          vs_bpe: 'BPE: Bottom-up (add tokens)\nUnigram: Top-down (remove tokens)\nUnigram often slightly better quality'
        },
        'sp-space': {
          title: '‚ñÅ SPACE HANDLING',
          content: 'Spaces encoded explicitly as ‚ñÅ (U+2581).',
          example: '"Hello world" ‚Üí ["‚ñÅHello", "‚ñÅworld"]\n"Hello  world" ‚Üí ["‚ñÅHello", "‚ñÅ", "‚ñÅworld"]',
          benefit: 'Preserves whitespace exactly\nNo ambiguity in decoding\nWorks for code (indentation matters!)'
        },
        'sp-training': {
          title: 'üéì TRAINING SENTENCEPIECE',
          content: 'Train tokenizer on raw text corpus.',
          code: 'import sentencepiece as spm\nspm.SentencePieceTrainer.train(\n  input="corpus.txt",\n  model_prefix="tokenizer",\n  vocab_size=32000,\n  model_type="bpe"  # or "unigram"\n)',
          params: 'vocab_size: 32K-256K typical\nmodel_type: bpe or unigram\ncharacter_coverage: 0.9995 typical'
        },

        // TIKTOKEN
        'tiktoken-concept': {
          title: 'üé´ TIKTOKEN',
          content: 'OpenAI\'s fast BPE tokenizer implementation.',
          features: 'Byte-level BPE\nExtremely fast (Rust backend)\nUsed by GPT-3.5, GPT-4',
          encodings: 'cl100k_base: GPT-4, ChatGPT\np50k_base: GPT-3\nr50k_base: Codex'
        },
        'tiktoken-usage': {
          title: 'üíª TIKTOKEN USAGE',
          content: 'Simple API for encoding and decoding.',
          code: 'import tiktoken\nenc = tiktoken.encoding_for_model("gpt-4")\ntokens = enc.encode("Hello world")\ntext = enc.decode(tokens)\nprint(len(tokens))  # Count tokens',
          tip: 'Use to estimate costs before API calls\n1000 tokens ‚âà 750 words'
        },

        // SPECIAL TOKENS
        'special-concept': {
          title: '‚≠ê SPECIAL TOKENS',
          content: 'Reserved tokens with special meaning for the model.',
          types: '<|endoftext|>: End of document\n<|im_start|>: Message start\n<|im_end|>: Message end\n[PAD]: Padding\n[UNK]: Unknown (rare)\n[CLS], [SEP]: BERT-style',
          important: 'Never split or merge special tokens\nAdded to vocab explicitly'
        },
        'special-chat': {
          title: 'üí¨ CHAT TEMPLATES',
          content: 'Special tokens structure multi-turn conversations.',
          example: '<|im_start|>system\nYou are helpful.<|im_end|>\n<|im_start|>user\nHello!<|im_end|>\n<|im_start|>assistant\nHi there!<|im_end|>',
          models: 'ChatML: <|im_start|>, <|im_end|>\nLLaMA: [INST], [/INST]\nMistral: Similar to LLaMA'
        },
        'special-bos-eos': {
          title: 'üèÅ BOS/EOS TOKENS',
          content: 'Mark beginning and end of sequences.',
          bos: 'BOS (Beginning of Sequence):\nSignals start of generation\nSome models require it',
          eos: 'EOS (End of Sequence):\nSignals generation complete\nModel learns to output this\nUsed as stop condition'
        },
        'special-pad': {
          title: 'üìè PADDING TOKEN',
          content: 'Fill sequences to same length for batching.',
          why: 'Batched inference needs same-length sequences\nPad shorter sequences to max length\nAttention mask ignores pad tokens',
          caution: 'Some models (GPT) have no pad token\nOften set pad_token = eos_token'
        },

        // VOCABULARY
        'vocab-size': {
          title: 'üìä VOCABULARY SIZE',
          content: 'Tradeoff between sequence length and embedding table size.',
          typical: 'GPT-2: 50,257\nGPT-4: 100,000+\nLLaMA: 32,000\nLLaMA-3: 128,000\nGemma: 256,000',
          tradeoff: 'Larger vocab: Shorter sequences, bigger embedding table\nSmaller vocab: Longer sequences, smaller embeddings'
        },
        'vocab-coverage': {
          title: 'üìà CHARACTER COVERAGE',
          content: 'What percentage of characters are in base vocab.',
          typical: '99.95% coverage common\nRemaining 0.05%: Rare Unicode, typos\nThese become <unk> or byte fallback',
          multilingual: 'Higher coverage needed for multilingual\nCJK needs many base characters'
        },
        'vocab-embedding': {
          title: 'üìê EMBEDDING TABLE',
          content: 'Each token gets a learned vector.',
          size: 'Table size: vocab_size √ó hidden_dim\nGPT-4: ~100K √ó 12288 = 1.2B params!\nLLaMA-70B: 32K √ó 8192 = 262M params',
          note: 'Larger vocab = more params in embeddings\nBut fewer tokens per sequence'
        },

        // TOKENIZATION EFFECTS
        'effect-fertility': {
          title: 'üìä FERTILITY',
          content: 'Average number of tokens per word. Varies by language.',
          english: 'English: ~1.3 tokens/word\nGerman: ~1.5 tokens/word\nChinese: ~2-3 tokens/char\nArabic: ~2 tokens/word',
          implication: 'Higher fertility = more tokens = higher cost\nNon-English often disadvantaged'
        },
        'effect-bias': {
          title: '‚öñÔ∏è TOKENIZER BIAS',
          content: 'Tokenizers trained mostly on English are biased.',
          problem: 'Common English words: 1 token\nCommon Chinese words: 2-4 tokens\nSame content costs more in other languages',
          solution: 'Multilingual training data\nLarger vocabularies\nLanguage-specific tokenizers'
        },
        'effect-code': {
          title: 'üíª CODE TOKENIZATION',
          content: 'Code has different patterns than natural language.',
          challenges: 'Whitespace significant (Python)\nLong identifiers: camelCase, snake_case\nSpecial characters: {{, }}, =>, ...',
          solution: 'Code-specific training data\nPreserve indentation\nTreat common patterns as tokens'
        },
        'effect-numbers': {
          title: 'üî¢ NUMBER TOKENIZATION',
          content: 'Numbers often tokenize inconsistently.',
          problem: '"123" might be ["1", "23"] or ["12", "3"]\nArithmetic harder when digits split\nLarge numbers very long',
          example: '"12345" ‚Üí ["123", "45"] (2 tokens)\n"12346" ‚Üí ["12", "34", "6"] (3 tokens)\nInconsistent!'
        },

        // PRACTICAL
        'practical-count': {
          title: 'üî¢ COUNTING TOKENS',
          content: 'Essential for context limits and cost estimation.',
          tools: 'tiktoken: OpenAI models\ntokenizers (HuggingFace): Most models\nOnline: OpenAI tokenizer playground',
          rule: 'Rough estimate: 1 token ‚âà 4 chars ‚âà 0.75 words\nBut varies by content!'
        },
        'practical-truncation': {
          title: '‚úÇÔ∏è TRUNCATION',
          content: 'Handle text longer than context window.',
          strategies: 'Truncate start: Keep recent context\nTruncate end: Keep beginning\nTruncate middle: Keep start and end\nChunking: Split into pieces',
          caution: 'Don\'t truncate mid-token!\nUse tokenizer\'s truncation methods'
        },
        'practical-prompt': {
          title: 'üìù PROMPT DESIGN',
          content: 'Tokenization affects prompt engineering.',
          tips: 'Check token count before sending\nLeave room for response\nCommon words = fewer tokens\nAvoid unusual Unicode if possible',
          example: 'GPT-4 context: 128K tokens\nYour prompt: 100K tokens\nResponse space: Only 28K tokens!'
        },

        // COMPARISON
        'compare-methods': {
          title: 'üìä METHOD COMPARISON',
          content: 'Different tokenization approaches for different needs.',
          comparison: 'BPE: Simple, widely used, GPT-style\nWordPiece: Google/BERT, ## markers\nSentencePiece: Language-agnostic, ‚ñÅ spaces\nUnigram: Alternative to BPE, often better',
          recommendation: 'Most LLMs: Byte-level BPE (tiktoken)\nBERT-style: WordPiece\nMultilingual: SentencePiece'
        },
        'compare-models': {
          title: 'ü§ñ MODEL TOKENIZERS',
          content: 'Which models use which tokenizers.',
          mapping: 'GPT-2/3/4: Byte-level BPE (tiktoken)\nLLaMA/Mistral: SentencePiece BPE\nBERT: WordPiece\nT5: SentencePiece Unigram\nClaude: Byte-level BPE variant',
          note: 'Can\'t mix tokenizers across models!\nEach model needs its own tokenizer'
        },

        // ADVANCED
        'advanced-pretokenize': {
          title: '‚úÇÔ∏è PRE-TOKENIZATION',
          content: 'Split text before subword tokenization.',
          methods: 'Whitespace split: Simple, common\nPunctuation split: Separate punctuation\nRegex patterns: Custom rules\nNone: SentencePiece style',
          example: '"Hello, world!" ‚Üí\nWhitespace: ["Hello,", "world!"]\nPunctuation: ["Hello", ",", "world", "!"]'
        },
        'advanced-normalization': {
          title: 'üîß NORMALIZATION',
          content: 'Standardize text before tokenization.',
          options: 'NFD/NFC: Unicode normalization\nLowercase: Case folding\nAccent removal: caf√© ‚Üí cafe\nWhitespace: Normalize spaces',
          caution: 'Some normalization loses information\nModel must match training normalization'
        },
        'advanced-added-tokens': {
          title: '‚ûï ADDING TOKENS',
          content: 'Extend vocabulary with custom tokens.',
          use_cases: 'Domain-specific terms\nUser/assistant markers\nTool call syntax\nSpecial formatting',
          code: 'tokenizer.add_tokens(["<CUSTOM>"])\nmodel.resize_token_embeddings(len(tokenizer))',
          caution: 'New tokens have random embeddings\nNeed fine-tuning to learn meaning'
        },
      };

      const Tooltip = () => {
        if (!tooltip || !tooltips[tooltip]) return null;
        const t = tooltips[tooltip];
        
        let left = mousePos.x + 15;
        let top = mousePos.y + 15;
        if (left + 420 > window.innerWidth) left = mousePos.x - 420;
        if (top + 350 > window.innerHeight) top = window.innerHeight - 360;
        
        return (
          <div 
            className="fixed z-50 w-[400px] p-5 bg-slate-900 border-2 border-white/20 rounded-xl shadow-2xl"
            style={{ left, top }}
          >
            <div className="text-lg font-black text-white mb-2">{t.title}</div>
            <p className="text-slate-300 text-sm leading-relaxed mb-3">{t.content}</p>
            {t.process && (
              <div className="bg-black/50 rounded-lg p-3 mb-3">
                <div className="text-xs text-slate-500 font-bold mb-1">PROCESS</div>
                <pre className="text-xs text-cyan-300 font-mono whitespace-pre-wrap">{t.process}</pre>
              </div>
            )}
            {t.steps && (
              <div className="bg-black/50 rounded-lg p-3 mb-3">
                <div className="text-xs text-slate-500 font-bold mb-1">STEPS</div>
                <pre className="text-xs text-green-300 font-mono whitespace-pre-wrap">{t.steps}</pre>
              </div>
            )}
            {t.example && (
              <div className="bg-black/50 rounded-lg p-3 mb-3">
                <div className="text-xs text-slate-500 font-bold mb-1">EXAMPLE</div>
                <pre className="text-xs text-yellow-300 font-mono whitespace-pre-wrap">{t.example}</pre>
              </div>
            )}
            {t.code && (
              <div className="bg-black/50 rounded-lg p-3 mb-3">
                <div className="text-xs text-slate-500 font-bold mb-1">CODE</div>
                <pre className="text-xs text-blue-300 font-mono whitespace-pre-wrap">{t.code}</pre>
              </div>
            )}
            {t.types && (
              <div className="bg-black/50 rounded-lg p-3 mb-3">
                <div className="text-xs text-slate-500 font-bold mb-1">TYPES</div>
                <pre className="text-xs text-purple-300 font-mono whitespace-pre-wrap">{t.types}</pre>
              </div>
            )}
            {t.typical && (
              <div className="bg-black/50 rounded-lg p-3 mb-3">
                <div className="text-xs text-slate-500 font-bold mb-1">TYPICAL VALUES</div>
                <pre className="text-xs text-orange-300 font-mono whitespace-pre-wrap">{t.typical}</pre>
              </div>
            )}
            {t.benefit && (
              <div className="text-xs text-green-400 mt-2">
                <span className="font-bold">‚úÖ </span>{t.benefit}
              </div>
            )}
            {t.tradeoff && (
              <div className="text-xs text-yellow-400 mt-2">
                <span className="font-bold">‚öñÔ∏è </span>{t.tradeoff}
              </div>
            )}
            {t.caution && (
              <div className="text-xs text-orange-400 mt-2">
                <span className="font-bold">‚ö†Ô∏è </span>{t.caution}
              </div>
            )}
          </div>
        );
      };

      const Hoverable = ({ id, children, className = '' }) => (
        <div 
          onMouseEnter={() => showTooltip(id)}
          onMouseLeave={hideTooltip}
          className={`cursor-pointer transition-all hover:scale-[1.02] hover:brightness-110 ${className}`}
        >
          {children}
        </div>
      );

      const SectionHeader = ({ number, title, subtitle, color }) => (
        <div className="flex items-center gap-4 mb-6">
          <div className={`w-14 h-14 rounded-xl bg-gradient-to-br ${color} flex items-center justify-center text-white text-2xl font-black shadow-lg`}>
            {number}
          </div>
          <div>
            <h2 className="text-3xl font-black text-white">{title}</h2>
            <p className="text-white/60">{subtitle}</p>
          </div>
        </div>
      );

      return (
        <div className="min-h-screen bg-black text-white p-8" onMouseMove={handleMouseMove}>
          <Tooltip />
          
          {/* Background */}
          <div className="fixed inset-0 pointer-events-none overflow-hidden">
            <div className="absolute w-[800px] h-[800px] -top-96 left-1/4 bg-amber-500/10 rounded-full blur-3xl" />
            <div className="absolute w-[600px] h-[600px] top-1/2 right-0 bg-orange-500/10 rounded-full blur-3xl" />
            <div className="absolute w-[600px] h-[600px] bottom-0 left-0 bg-yellow-500/10 rounded-full blur-3xl" />
          </div>

          <div className="relative max-w-7xl mx-auto">
            {/* Header */}
            <header className="text-center mb-12">
              <div className="inline-flex items-center gap-2 px-6 py-2 bg-amber-600/30 border border-amber-400 rounded-full mb-6">
                <span className="text-amber-300 font-bold">TOKENIZATION</span>
                <span className="text-white">‚Ä¢</span>
                <span className="text-amber-200 font-medium">Hover for details</span>
              </div>
              <h1 className="text-5xl font-black mb-4 text-transparent bg-clip-text bg-gradient-to-r from-amber-400 via-orange-400 to-yellow-400">
                Tokenization Deep Dive
              </h1>
              <p className="text-xl text-slate-200 max-w-3xl mx-auto leading-relaxed">
                <span className="text-amber-300 font-bold">Text ‚Üí Tokens ‚Üí IDs</span> ‚Äî the first step in every LLM pipeline.
              </p>
            </header>

            {/* Core Concept */}
            <div className="bg-gradient-to-r from-amber-900/50 to-orange-900/50 border-2 border-amber-500/50 rounded-xl p-8 mb-12">
              <div className="text-center mb-6">
                <div className="text-2xl font-black text-white">üî§ THE TOKENIZATION FLOW</div>
              </div>
              <div className="flex items-center justify-center gap-4 flex-wrap">
                <div className="px-6 py-4 rounded-lg bg-slate-800 border border-slate-600">
                  <div className="text-slate-400 text-xs mb-1">Input</div>
                  <div className="text-white font-mono text-lg">"Hello world"</div>
                </div>
                <span className="text-amber-400 text-2xl">‚Üí</span>
                <div className="px-6 py-4 rounded-lg bg-amber-900/50 border border-amber-500">
                  <div className="text-amber-400 text-xs mb-1">Tokens</div>
                  <div className="text-amber-200 font-mono text-lg">["Hello", " world"]</div>
                </div>
                <span className="text-amber-400 text-2xl">‚Üí</span>
                <div className="px-6 py-4 rounded-lg bg-orange-900/50 border border-orange-500">
                  <div className="text-orange-400 text-xs mb-1">Token IDs</div>
                  <div className="text-orange-200 font-mono text-lg">[15496, 995]</div>
                </div>
                <span className="text-amber-400 text-2xl">‚Üí</span>
                <div className="px-6 py-4 rounded-lg bg-yellow-900/50 border border-yellow-500">
                  <div className="text-yellow-400 text-xs mb-1">Embeddings</div>
                  <div className="text-yellow-200 font-mono text-lg">[vec‚ÇÅ, vec‚ÇÇ]</div>
                </div>
              </div>
            </div>

            {/* WHY SUBWORDS */}
            <section className="mb-10">
              <SectionHeader 
                number="1" 
                title="Why Subwords?" 
                subtitle="The case for subword tokenization"
                color="from-amber-500 to-yellow-600"
              />
              
              <div className="grid grid-cols-4 gap-4">
                <Hoverable id="why-not-chars">
                  <div className="p-4 rounded-xl bg-red-900/40 border-2 border-red-400">
                    <div className="text-lg font-black text-red-200 mb-2">‚ùå Characters</div>
                    <div className="text-red-100/70 text-sm">Too many tokens</div>
                    <div className="text-red-300 text-xs mt-2">5000 chars vs 1300 subwords</div>
                  </div>
                </Hoverable>

                <Hoverable id="why-not-words">
                  <div className="p-4 rounded-xl bg-red-900/40 border-2 border-red-400">
                    <div className="text-lg font-black text-red-200 mb-2">‚ùå Words</div>
                    <div className="text-red-100/70 text-sm">OOV problem</div>
                    <div className="text-red-300 text-xs mt-2">Can't handle new words</div>
                  </div>
                </Hoverable>

                <Hoverable id="why-subwords">
                  <div className="p-4 rounded-xl bg-green-900/40 border-2 border-green-400">
                    <div className="text-lg font-black text-green-200 mb-2">‚úÖ Subwords</div>
                    <div className="text-green-100/70 text-sm">Best of both</div>
                    <div className="text-green-300 text-xs mt-2">Fixed vocab, any text</div>
                  </div>
                </Hoverable>

                <Hoverable id="why-tokenize">
                  <div className="p-4 rounded-xl bg-amber-900/40 border-2 border-amber-400">
                    <div className="text-lg font-black text-amber-200 mb-2">üéØ Goal</div>
                    <div className="text-amber-100/70 text-sm">Text ‚Üí Numbers</div>
                    <div className="text-amber-300 text-xs mt-2">NNs need vectors</div>
                  </div>
                </Hoverable>
              </div>
            </section>

            {/* BPE */}
            <section className="mb-10">
              <SectionHeader 
                number="2" 
                title="Byte Pair Encoding (BPE)" 
                subtitle="The most common tokenization algorithm"
                color="from-cyan-500 to-blue-600"
              />
              
              <div className="bg-slate-900/60 border border-cyan-500/30 rounded-xl p-6 mb-4">
                <Hoverable id="bpe-concept">
                  <div className="text-center mb-4">
                    <div className="text-cyan-300 font-bold text-lg">BPE Algorithm</div>
                    <div className="text-cyan-100/70 text-sm">Start with characters, merge frequent pairs</div>
                  </div>
                </Hoverable>

                <div className="grid grid-cols-4 gap-3 mb-4">
                  <Hoverable id="bpe-training">
                    <div className="p-3 rounded-lg bg-cyan-900/50 border border-cyan-400 text-center">
                      <div className="text-cyan-200 font-bold text-sm">üéì Training</div>
                      <div className="text-cyan-100/60 text-xs">Learn merge rules</div>
                    </div>
                  </Hoverable>
                  <Hoverable id="bpe-merge-rules">
                    <div className="p-3 rounded-lg bg-blue-900/50 border border-blue-400 text-center">
                      <div className="text-blue-200 font-bold text-sm">üìã Merge Rules</div>
                      <div className="text-blue-100/60 text-xs">Ordered list</div>
                    </div>
                  </Hoverable>
                  <Hoverable id="bpe-encoding">
                    <div className="p-3 rounded-lg bg-indigo-900/50 border border-indigo-400 text-center">
                      <div className="text-indigo-200 font-bold text-sm">üî¢ Encoding</div>
                      <div className="text-indigo-100/60 text-xs">Apply rules</div>
                    </div>
                  </Hoverable>
                  <Hoverable id="bpe-decoding">
                    <div className="p-3 rounded-lg bg-violet-900/50 border border-violet-400 text-center">
                      <div className="text-violet-200 font-bold text-sm">üîô Decoding</div>
                      <div className="text-violet-100/60 text-xs">Concatenate</div>
                    </div>
                  </Hoverable>
                </div>

                {/* BPE Example */}
                <div className="bg-black/50 rounded-lg p-4">
                  <div className="text-sm text-slate-400 mb-2">BPE Example: "lowest"</div>
                  <div className="font-mono text-sm space-y-1">
                    <div className="text-slate-500">Start: <span className="text-white">l o w e s t</span></div>
                    <div className="text-cyan-400">Merge (l,o): <span className="text-white">lo w e s t</span></div>
                    <div className="text-blue-400">Merge (lo,w): <span className="text-white">low e s t</span></div>
                    <div className="text-indigo-400">Merge (e,s): <span className="text-white">low es t</span></div>
                    <div className="text-green-400">Result: <span className="text-white">["low", "es", "t"]</span></div>
                  </div>
                </div>
              </div>
            </section>

            {/* BYTE-LEVEL BPE */}
            <section className="mb-10">
              <SectionHeader 
                number="3" 
                title="Byte-Level BPE" 
                subtitle="Used by GPT-2/3/4, LLaMA, Mistral"
                color="from-green-500 to-emerald-600"
              />
              
              <div className="grid grid-cols-3 gap-4">
                <Hoverable id="bbpe-concept">
                  <div className="p-5 rounded-xl bg-green-900/40 border-2 border-green-400">
                    <div className="text-xl font-black text-green-200 mb-2">üî¢ Byte-Level</div>
                    <div className="text-green-100/70 text-sm">Operate on bytes (0-255)</div>
                    <div className="text-green-300 text-xs mt-2">Never OOV!</div>
                  </div>
                </Hoverable>

                <Hoverable id="bbpe-base-vocab">
                  <div className="p-5 rounded-xl bg-emerald-900/40 border-2 border-emerald-400">
                    <div className="text-xl font-black text-emerald-200 mb-2">üìä Base Vocab</div>
                    <div className="text-emerald-100/70 text-sm">256 byte tokens</div>
                    <div className="text-emerald-300 text-xs mt-2">+ merged tokens</div>
                  </div>
                </Hoverable>

                <Hoverable id="bbpe-utf8">
                  <div className="p-5 rounded-xl bg-teal-900/40 border-2 border-teal-400">
                    <div className="text-xl font-black text-teal-200 mb-2">üåê UTF-8</div>
                    <div className="text-teal-100/70 text-sm">Text ‚Üí UTF-8 ‚Üí BPE</div>
                    <div className="text-teal-300 text-xs mt-2">Any language works</div>
                  </div>
                </Hoverable>
              </div>
            </section>

            {/* WORDPIECE & SENTENCEPIECE */}
            <section className="mb-10">
              <SectionHeader 
                number="4" 
                title="Other Methods" 
                subtitle="WordPiece, SentencePiece, Unigram"
                color="from-purple-500 to-violet-600"
              />
              
              <div className="grid grid-cols-2 gap-6 mb-4">
                {/* WordPiece */}
                <div className="bg-purple-900/30 border border-purple-500/50 rounded-xl p-5">
                  <Hoverable id="wordpiece-concept">
                    <div className="text-xl font-black text-purple-300 mb-3">üî§ WordPiece</div>
                  </Hoverable>
                  <div className="text-purple-100/70 text-sm mb-4">Google/BERT style with ## markers</div>
                  <div className="grid grid-cols-2 gap-2">
                    <Hoverable id="wordpiece-training">
                      <div className="bg-purple-900/50 rounded-lg p-2 text-center">
                        <div className="text-purple-200 text-xs font-bold">Likelihood</div>
                        <div className="text-purple-100/60 text-xs">Not just frequency</div>
                      </div>
                    </Hoverable>
                    <Hoverable id="wordpiece-prefix">
                      <div className="bg-purple-900/50 rounded-lg p-2 text-center">
                        <div className="text-purple-200 text-xs font-bold">## Prefix</div>
                        <div className="text-purple-100/60 text-xs">Continuation marker</div>
                      </div>
                    </Hoverable>
                  </div>
                  <div className="mt-3 text-xs text-purple-400 font-mono bg-black/30 p-2 rounded">
                    "playing" ‚Üí ["play", "##ing"]
                  </div>
                </div>

                {/* SentencePiece */}
                <div className="bg-violet-900/30 border border-violet-500/50 rounded-xl p-5">
                  <Hoverable id="sp-concept">
                    <div className="text-xl font-black text-violet-300 mb-3">üî§ SentencePiece</div>
                  </Hoverable>
                  <div className="text-violet-100/70 text-sm mb-4">Language-agnostic, ‚ñÅ for spaces</div>
                  <div className="grid grid-cols-2 gap-2">
                    <Hoverable id="sp-unigram">
                      <div className="bg-violet-900/50 rounded-lg p-2 text-center">
                        <div className="text-violet-200 text-xs font-bold">Unigram</div>
                        <div className="text-violet-100/60 text-xs">Top-down approach</div>
                      </div>
                    </Hoverable>
                    <Hoverable id="sp-space">
                      <div className="bg-violet-900/50 rounded-lg p-2 text-center">
                        <div className="text-violet-200 text-xs font-bold">‚ñÅ Spaces</div>
                        <div className="text-violet-100/60 text-xs">Explicit encoding</div>
                      </div>
                    </Hoverable>
                  </div>
                  <div className="mt-3 text-xs text-violet-400 font-mono bg-black/30 p-2 rounded">
                    "Hello world" ‚Üí ["‚ñÅHello", "‚ñÅworld"]
                  </div>
                </div>
              </div>

              <Hoverable id="sp-training">
                <div className="bg-slate-800 rounded-lg p-3">
                  <div className="text-slate-200 font-bold text-sm mb-1">üíª Training SentencePiece</div>
                  <div className="font-mono text-xs text-slate-400">
                    spm.SentencePieceTrainer.train(input="corpus.txt", vocab_size=32000, model_type="bpe")
                  </div>
                </div>
              </Hoverable>
            </section>

            {/* TIKTOKEN */}
            <section className="mb-10">
              <SectionHeader 
                number="5" 
                title="Tiktoken" 
                subtitle="OpenAI's fast BPE tokenizer"
                color="from-sky-500 to-blue-600"
              />
              
              <div className="grid grid-cols-2 gap-4">
                <Hoverable id="tiktoken-concept">
                  <div className="p-5 rounded-xl bg-sky-900/40 border-2 border-sky-400">
                    <div className="text-xl font-black text-sky-200 mb-2">üé´ Tiktoken</div>
                    <div className="text-sky-100/70 text-sm">Fast Rust backend</div>
                    <div className="text-sky-300 text-xs mt-2">cl100k_base for GPT-4</div>
                  </div>
                </Hoverable>

                <Hoverable id="tiktoken-usage">
                  <div className="p-5 rounded-xl bg-blue-900/40 border-2 border-blue-400">
                    <div className="text-xl font-black text-blue-200 mb-2">üíª Usage</div>
                    <div className="font-mono text-xs text-blue-300 bg-black/30 p-2 rounded">
                      enc = tiktoken.encoding_for_model("gpt-4")<br/>
                      tokens = enc.encode("Hello world")
                    </div>
                  </div>
                </Hoverable>
              </div>
            </section>

            {/* SPECIAL TOKENS */}
            <section className="mb-10">
              <SectionHeader 
                number="6" 
                title="Special Tokens" 
                subtitle="Reserved tokens with special meaning"
                color="from-pink-500 to-rose-600"
              />
              
              <div className="grid grid-cols-4 gap-3 mb-4">
                <Hoverable id="special-concept">
                  <div className="p-4 rounded-xl bg-pink-900/50 border-2 border-pink-400">
                    <div className="text-lg font-black text-pink-200 mb-2">‚≠ê Special</div>
                    <div className="text-pink-100/70 text-sm">Reserved tokens</div>
                  </div>
                </Hoverable>

                <Hoverable id="special-chat">
                  <div className="p-4 rounded-xl bg-rose-900/50 border-2 border-rose-400">
                    <div className="text-lg font-black text-rose-200 mb-2">üí¨ Chat</div>
                    <div className="text-rose-100/70 text-sm">im_start, im_end</div>
                  </div>
                </Hoverable>

                <Hoverable id="special-bos-eos">
                  <div className="p-4 rounded-xl bg-red-900/50 border-2 border-red-400">
                    <div className="text-lg font-black text-red-200 mb-2">üèÅ BOS/EOS</div>
                    <div className="text-red-100/70 text-sm">Start/end markers</div>
                  </div>
                </Hoverable>

                <Hoverable id="special-pad">
                  <div className="p-4 rounded-xl bg-orange-900/50 border-2 border-orange-400">
                    <div className="text-lg font-black text-orange-200 mb-2">üìè PAD</div>
                    <div className="text-orange-100/70 text-sm">Batch padding</div>
                  </div>
                </Hoverable>
              </div>

              <div className="bg-slate-800 rounded-lg p-4">
                <div className="text-sm text-slate-400 mb-2">Chat Template Example</div>
                <div className="font-mono text-xs text-slate-300">
                  <span className="text-pink-400">&lt;|im_start|&gt;</span>system<br/>
                  You are helpful.<span className="text-pink-400">&lt;|im_end|&gt;</span><br/>
                  <span className="text-pink-400">&lt;|im_start|&gt;</span>user<br/>
                  Hello!<span className="text-pink-400">&lt;|im_end|&gt;</span><br/>
                  <span className="text-pink-400">&lt;|im_start|&gt;</span>assistant<br/>
                </div>
              </div>
            </section>

            {/* VOCABULARY */}
            <section className="mb-10">
              <SectionHeader 
                number="7" 
                title="Vocabulary" 
                subtitle="Size, coverage, and embedding table"
                color="from-indigo-500 to-violet-600"
              />
              
              <div className="grid grid-cols-3 gap-4 mb-4">
                <Hoverable id="vocab-size">
                  <div className="p-4 rounded-xl bg-indigo-900/50 border-2 border-indigo-400">
                    <div className="text-lg font-black text-indigo-200 mb-2">üìä Vocab Size</div>
                    <div className="text-indigo-100/70 text-xs space-y-1">
                      <div>GPT-2: 50,257</div>
                      <div>LLaMA: 32,000</div>
                      <div>GPT-4: 100,000+</div>
                      <div>LLaMA-3: 128,000</div>
                    </div>
                  </div>
                </Hoverable>

                <Hoverable id="vocab-coverage">
                  <div className="p-4 rounded-xl bg-violet-900/50 border-2 border-violet-400">
                    <div className="text-lg font-black text-violet-200 mb-2">üìà Coverage</div>
                    <div className="text-violet-100/70 text-sm">99.95% typical</div>
                    <div className="text-violet-300 text-xs mt-2">Rare chars ‚Üí bytes</div>
                  </div>
                </Hoverable>

                <Hoverable id="vocab-embedding">
                  <div className="p-4 rounded-xl bg-purple-900/50 border-2 border-purple-400">
                    <div className="text-lg font-black text-purple-200 mb-2">üìê Embedding</div>
                    <div className="text-purple-100/70 text-sm">vocab √ó hidden_dim</div>
                    <div className="text-purple-300 text-xs mt-2">Can be 1B+ params</div>
                  </div>
                </Hoverable>
              </div>
            </section>

            {/* EFFECTS */}
            <section className="mb-10">
              <SectionHeader 
                number="8" 
                title="Tokenization Effects" 
                subtitle="How tokenization impacts model behavior"
                color="from-red-500 to-orange-600"
              />
              
              <div className="grid grid-cols-4 gap-3">
                <Hoverable id="effect-fertility">
                  <div className="p-3 rounded-lg bg-red-900/50 border border-red-400 text-center">
                    <div className="text-red-200 font-bold text-sm">üìä Fertility</div>
                    <div className="text-red-100/60 text-xs">Tokens per word</div>
                    <div className="text-red-300 text-xs mt-1">EN: 1.3, CN: 2-3</div>
                  </div>
                </Hoverable>

                <Hoverable id="effect-bias">
                  <div className="p-3 rounded-lg bg-orange-900/50 border border-orange-400 text-center">
                    <div className="text-orange-200 font-bold text-sm">‚öñÔ∏è Bias</div>
                    <div className="text-orange-100/60 text-xs">English-centric</div>
                    <div className="text-orange-300 text-xs mt-1">Other langs cost more</div>
                  </div>
                </Hoverable>

                <Hoverable id="effect-code">
                  <div className="p-3 rounded-lg bg-amber-900/50 border border-amber-400 text-center">
                    <div className="text-amber-200 font-bold text-sm">üíª Code</div>
                    <div className="text-amber-100/60 text-xs">Special patterns</div>
                    <div className="text-amber-300 text-xs mt-1">Whitespace matters</div>
                  </div>
                </Hoverable>

                <Hoverable id="effect-numbers">
                  <div className="p-3 rounded-lg bg-yellow-900/50 border border-yellow-400 text-center">
                    <div className="text-yellow-200 font-bold text-sm">üî¢ Numbers</div>
                    <div className="text-yellow-100/60 text-xs">Inconsistent splits</div>
                    <div className="text-yellow-300 text-xs mt-1">Math harder</div>
                  </div>
                </Hoverable>
              </div>
            </section>

            {/* PRACTICAL */}
            <section className="mb-10">
              <SectionHeader 
                number="9" 
                title="Practical Usage" 
                subtitle="Counting, truncation, prompt design"
                color="from-emerald-500 to-teal-600"
              />
              
              <div className="grid grid-cols-3 gap-4">
                <Hoverable id="practical-count">
                  <div className="p-4 rounded-xl bg-emerald-900/50 border-2 border-emerald-400">
                    <div className="text-lg font-black text-emerald-200 mb-2">üî¢ Counting</div>
                    <div className="text-emerald-100/70 text-sm">1 token ‚âà 4 chars</div>
                    <div className="text-emerald-300 text-xs mt-2">‚âà 0.75 words</div>
                  </div>
                </Hoverable>

                <Hoverable id="practical-truncation">
                  <div className="p-4 rounded-xl bg-teal-900/50 border-2 border-teal-400">
                    <div className="text-lg font-black text-teal-200 mb-2">‚úÇÔ∏è Truncation</div>
                    <div className="text-teal-100/70 text-sm">Handle long text</div>
                    <div className="text-teal-300 text-xs mt-2">Start, end, or middle</div>
                  </div>
                </Hoverable>

                <Hoverable id="practical-prompt">
                  <div className="p-4 rounded-xl bg-cyan-900/50 border-2 border-cyan-400">
                    <div className="text-lg font-black text-cyan-200 mb-2">üìù Prompts</div>
                    <div className="text-cyan-100/70 text-sm">Leave room for response</div>
                    <div className="text-cyan-300 text-xs mt-2">Check count first</div>
                  </div>
                </Hoverable>
              </div>
            </section>

            {/* COMPARISON */}
            <section className="mb-10">
              <Hoverable id="compare-models">
                <div className="bg-slate-900/60 border border-amber-500/30 rounded-xl p-6">
                  <div className="text-lg font-black text-amber-300 mb-4 text-center">ü§ñ MODEL TOKENIZERS</div>
                  <div className="overflow-x-auto">
                    <table className="w-full text-sm">
                      <thead>
                        <tr className="text-slate-400 border-b border-slate-700">
                          <th className="text-left p-2">Model</th>
                          <th className="text-center p-2">Method</th>
                          <th className="text-center p-2">Vocab Size</th>
                          <th className="text-center p-2">Library</th>
                        </tr>
                      </thead>
                      <tbody className="text-slate-300">
                        <tr className="border-b border-slate-800">
                          <td className="p-2 font-bold text-cyan-300">GPT-4</td>
                          <td className="text-center p-2">Byte BPE</td>
                          <td className="text-center p-2">100K+</td>
                          <td className="text-center p-2">tiktoken</td>
                        </tr>
                        <tr className="border-b border-slate-800">
                          <td className="p-2 font-bold text-green-300">LLaMA-3</td>
                          <td className="text-center p-2">SentencePiece</td>
                          <td className="text-center p-2">128K</td>
                          <td className="text-center p-2">tokenizers</td>
                        </tr>
                        <tr className="border-b border-slate-800">
                          <td className="p-2 font-bold text-purple-300">BERT</td>
                          <td className="text-center p-2">WordPiece</td>
                          <td className="text-center p-2">30K</td>
                          <td className="text-center p-2">tokenizers</td>
                        </tr>
                        <tr>
                          <td className="p-2 font-bold text-orange-300">Claude</td>
                          <td className="text-center p-2">Byte BPE</td>
                          <td className="text-center p-2">~100K</td>
                          <td className="text-center p-2">Custom</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>
              </Hoverable>
            </section>

            {/* ADVANCED */}
            <section className="mb-10">
              <SectionHeader 
                number="10" 
                title="Advanced Topics" 
                subtitle="Pre-tokenization, normalization, custom tokens"
                color="from-slate-500 to-zinc-600"
              />
              
              <div className="grid grid-cols-3 gap-4">
                <Hoverable id="advanced-pretokenize">
                  <div className="p-4 rounded-xl bg-slate-800 border-2 border-slate-500">
                    <div className="text-lg font-black text-slate-200 mb-2">‚úÇÔ∏è Pre-tokenize</div>
                    <div className="text-slate-400 text-sm">Split before BPE</div>
                    <div className="text-slate-500 text-xs mt-2">Whitespace, punctuation</div>
                  </div>
                </Hoverable>

                <Hoverable id="advanced-normalization">
                  <div className="p-4 rounded-xl bg-zinc-800 border-2 border-zinc-500">
                    <div className="text-lg font-black text-zinc-200 mb-2">üîß Normalize</div>
                    <div className="text-zinc-400 text-sm">Standardize text</div>
                    <div className="text-zinc-500 text-xs mt-2">NFC, lowercase, etc.</div>
                  </div>
                </Hoverable>

                <Hoverable id="advanced-added-tokens">
                  <div className="p-4 rounded-xl bg-stone-800 border-2 border-stone-500">
                    <div className="text-lg font-black text-stone-200 mb-2">‚ûï Add Tokens</div>
                    <div className="text-stone-400 text-sm">Extend vocabulary</div>
                    <div className="text-stone-500 text-xs mt-2">Custom, domain-specific</div>
                  </div>
                </Hoverable>
              </div>
            </section>

            {/* Summary */}
            <div className="bg-gradient-to-r from-amber-900/50 to-orange-900/50 border-2 border-amber-400/50 rounded-xl p-8 text-center">
              <div className="text-2xl font-black text-white mb-4">
                üî§ TOKENIZATION SUMMARY
              </div>
              <div className="text-slate-300 max-w-3xl mx-auto mb-6">
                Subword tokenization (BPE) balances vocabulary size with sequence length.
                Byte-level BPE handles any text. Special tokens structure conversations.
              </div>
              
              <div className="grid grid-cols-5 gap-3 mt-6 text-sm">
                <div className="bg-slate-800 rounded-lg p-3">
                  <div className="text-cyan-300 font-bold">BPE</div>
                  <div className="text-slate-400">Most common</div>
                </div>
                <div className="bg-slate-800 rounded-lg p-3">
                  <div className="text-green-300 font-bold">Byte-level</div>
                  <div className="text-slate-400">Any text</div>
                </div>
                <div className="bg-slate-800 rounded-lg p-3">
                  <div className="text-purple-300 font-bold">32K-256K</div>
                  <div className="text-slate-400">Vocab size</div>
                </div>
                <div className="bg-slate-800 rounded-lg p-3">
                  <div className="text-pink-300 font-bold">Special</div>
                  <div className="text-slate-400">BOS, EOS, PAD</div>
                </div>
                <div className="bg-slate-800 rounded-lg p-3">
                  <div className="text-amber-300 font-bold">~4 chars</div>
                  <div className="text-slate-400">Per token</div>
                </div>
              </div>
            </div>

            {/* Footer */}
            <footer className="mt-12 text-center text-slate-500 text-sm">
              <p>Tokenization Deep Dive ‚Ä¢ BPE ‚Ä¢ WordPiece ‚Ä¢ SentencePiece</p>
              <p className="text-xs mt-1 text-slate-600">Text ‚Üí Tokens ‚Üí IDs ‚Üí Embeddings ‚Üí Model</p>
            </footer>
          </div>
        </div>
      );
    }

    const root = ReactDOM.createRoot(document.getElementById('root'));
    root.render(<TokenizationDiagram />);
  </script>
</body>
</html>
