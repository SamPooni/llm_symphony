<!DOCTYPE html>
<html>
<head>
  <!-- © 2026 Subramaniyam Pooni | CS²B Technologies (CSSQUAREDB Technologies) | All Rights Reserved -->
  <meta charset="UTF-8">
  <meta name="author" content="Subramaniyam Pooni">
  <meta name="company" content="CS²B Technologies (CSSQUAREDB Technologies)">
  <title>Edge Deployment - Ollama, llama.cpp, MLX</title>
  <script src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
  <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body style="margin:0;background:#000">
  <div id="root"></div>
  <script type="text/babel">
    function App() {
      const [hover, setHover] = React.useState(null);
      
      const tips = {
        'why': 'Run LLMs locally. Privacy: data stays on device. Cost: no API fees. Latency: no network round-trip. Offline: works without internet.',
        'ollama': 'Ollama. Docker-like for LLMs. Simple CLI. Model library. Mac/Linux/Windows. Most popular local option. Just works.',
        'llamacpp': 'llama.cpp. C++ inference engine. CPU optimized. GGUF format. Foundation for most local tools. Ggerganovs masterpiece.',
        'mlx': 'Apple MLX. Metal-optimized for Apple Silicon. M1/M2/M3 native. Fast on Mac. Python API. Growing ecosystem.',
        'vllm-local': 'vLLM locally. Production-grade serving on your GPU. PagedAttention benefits. High throughput local deployment.',
        'gguf': 'GGUF format. Quantized model format. 4-bit, 8-bit variants. CPU-friendly. Standard for local deployment.',
        'quant': 'Quantization essential. 70B at 4-bit fits in 40GB. Quality vs size tradeoff. Q4_K_M popular balance.',
        'cpu': 'CPU inference. Slower but works anywhere. llama.cpp optimized. Good for small models. No GPU needed.',
        'gpu': 'GPU inference. Much faster. CUDA, Metal, ROCm. Need VRAM for model. Consumer GPUs work for 7B-13B.',
        'mobile': 'Mobile LLMs. MLC LLM, llama.cpp on iOS/Android. On-device inference. Privacy-preserving. Battery considerations.',
        'models': 'Popular local models. LLaMA, Mistral, Phi, Gemma, Qwen. Open weights. Various sizes. Check license.',
        'memory': 'Memory requirements. ~1GB per billion parameters at 4-bit. 7B = 4-5GB. 70B = 40GB. Check your RAM.',
        'hybrid': 'Hybrid: local + cloud. Simple queries local, hard queries cloud. Best of both. Router decides.',
        'privacy': 'Privacy benefit. Data never leaves device. Healthcare, legal, sensitive use cases. Compliance friendly.'
      };

      const Card = ({id, bg, title, sub}) => (
        <div 
          className={`p-4 rounded-xl border-2 ${bg} cursor-pointer hover:scale-105 transition-transform relative`}
          onMouseEnter={() => setHover(id)}
          onMouseLeave={() => setHover(null)}
        >
          <div className="font-bold">{title}</div>
          <div className="text-sm opacity-60">{sub}</div>
          {hover === id && (
            <div className="absolute left-0 top-full mt-2 z-50 w-72 p-3 bg-slate-800 border border-cyan-500 rounded-lg text-sm text-white shadow-xl">
              {tips[id]}
            </div>
          )}
        </div>
      );

      return (
        <div className="min-h-screen bg-black text-white p-8">
          <div className="max-w-5xl mx-auto">
            <div className="text-center mb-8">
              <div className="inline-block px-4 py-1 bg-teal-900/50 border border-teal-500 rounded-full mb-4">
                <span className="text-teal-300 font-bold text-sm">LOCAL DEPLOYMENT</span>
              </div>
              <h1 className="text-4xl font-black mb-2 text-transparent bg-clip-text bg-gradient-to-r from-teal-400 to-cyan-500">
                Edge Deployment
              </h1>
              <p className="text-slate-400">Run LLMs locally with Ollama, llama.cpp, MLX. Hover for details.</p>
            </div>

            {/* Why Local */}
            <div className="mb-8">
              <h2 className="text-lg font-bold mb-3 text-teal-300">1. Why Local?</h2>
              <div className="grid grid-cols-2 gap-3">
                <Card id="why" bg="bg-teal-900/50 border-teal-500 text-teal-200" title="Benefits" sub="Privacy, cost, latency"/>
                <Card id="privacy" bg="bg-teal-900/50 border-teal-500 text-teal-200" title="Privacy" sub="Data stays on device"/>
              </div>
            </div>

            {/* Tools */}
            <div className="mb-8">
              <h2 className="text-lg font-bold mb-3 text-blue-300">2. Tools</h2>
              <div className="grid grid-cols-4 gap-3">
                <Card id="ollama" bg="bg-blue-900/50 border-blue-500 text-blue-200" title="Ollama" sub="Docker for LLMs"/>
                <Card id="llamacpp" bg="bg-blue-900/50 border-blue-500 text-blue-200" title="llama.cpp" sub="C++ inference"/>
                <Card id="mlx" bg="bg-blue-900/50 border-blue-500 text-blue-200" title="MLX" sub="Apple Silicon"/>
                <Card id="vllm-local" bg="bg-blue-900/50 border-blue-500 text-blue-200" title="vLLM Local" sub="Production serving"/>
              </div>
            </div>

            {/* Formats & Quantization */}
            <div className="mb-8">
              <h2 className="text-lg font-bold mb-3 text-purple-300">3. Formats</h2>
              <div className="grid grid-cols-3 gap-3">
                <Card id="gguf" bg="bg-purple-900/50 border-purple-500 text-purple-200" title="GGUF" sub="Standard format"/>
                <Card id="quant" bg="bg-purple-900/50 border-purple-500 text-purple-200" title="Quantization" sub="4-bit, 8-bit"/>
                <Card id="memory" bg="bg-purple-900/50 border-purple-500 text-purple-200" title="Memory" sub="1GB per billion"/>
              </div>
            </div>

            {/* Hardware */}
            <div className="mb-8">
              <h2 className="text-lg font-bold mb-3 text-orange-300">4. Hardware</h2>
              <div className="grid grid-cols-3 gap-3">
                <Card id="cpu" bg="bg-orange-900/50 border-orange-500 text-orange-200" title="CPU" sub="Slower but works"/>
                <Card id="gpu" bg="bg-orange-900/50 border-orange-500 text-orange-200" title="GPU" sub="Fast, needs VRAM"/>
                <Card id="mobile" bg="bg-orange-900/50 border-orange-500 text-orange-200" title="Mobile" sub="iOS/Android"/>
              </div>
            </div>

            {/* Models & Strategy */}
            <div className="mb-8">
              <h2 className="text-lg font-bold mb-3 text-green-300">5. Models & Strategy</h2>
              <div className="grid grid-cols-2 gap-3">
                <Card id="models" bg="bg-green-900/50 border-green-500 text-green-200" title="Models" sub="LLaMA, Mistral, Phi"/>
                <Card id="hybrid" bg="bg-green-900/50 border-green-500 text-green-200" title="Hybrid" sub="Local + cloud"/>
              </div>
            </div>

            {/* Summary */}
            <div className="bg-slate-900/50 border border-teal-500/30 rounded-xl p-6 text-center">
              <div className="text-xl font-bold text-white mb-2">Edge Deployment Summary</div>
              <p className="text-slate-400 text-sm mb-4">
                Ollama for easy start, llama.cpp under the hood, GGUF for format, 4-bit for memory.
              </p>
              <div className="flex justify-center gap-3 flex-wrap">
                <span className="px-3 py-1 bg-blue-900/50 rounded text-blue-300 text-sm">Ollama</span>
                <span className="px-3 py-1 bg-purple-900/50 rounded text-purple-300 text-sm">GGUF</span>
                <span className="px-3 py-1 bg-orange-900/50 rounded text-orange-300 text-sm">4-bit Quant</span>
                <span className="px-3 py-1 bg-green-900/50 rounded text-green-300 text-sm">Hybrid</span>
              </div>
            </div>
          </div>
        </div>
      );
    }
    ReactDOM.createRoot(document.getElementById('root')).render(<App />);
  </script>
  <div id="cs2b-copyright-footer" style="position:fixed;bottom:0;left:0;right:0;background:linear-gradient(to right,#0f172a,#1e1b4b);border-top:1px solid #334155;padding:8px 16px;font-family:monospace;font-size:11px;color:#64748b;display:flex;justify-content:space-between;align-items:center;z-index:9999;"><span>© 2026 Subramaniyam Pooni</span><span style="color:#8b5cf6;">CS²B Technologies</span><span style="color:#475569;">Enterprise AI Agent Development &amp; LLMOps</span></div>
<div id="cs2b-watermark" style="position:fixed;top:50%;left:50%;transform:translate(-50%,-50%) rotate(-35deg);pointer-events:none;z-index:9998;white-space:nowrap;user-select:none;text-align:center;"><div style="font-size:100px;font-weight:900;font-family:Arial,sans-serif;color:rgba(139,92,246,0.06);">CS²B Technologies</div><div style="font-size:40px;font-weight:700;font-family:Arial,sans-serif;color:rgba(139,92,246,0.05);">© Subramaniyam Pooni</div></div>
</body>
</html>
